{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e284ec2",
      "metadata": {
        "id": "6e284ec2"
      },
      "source": [
        "### NOTE: The Final Scaling technique will be decided once we get the dataset.Till then all the major scaling techniques are listed below. We shall update more techniques if required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd4bfa9a",
      "metadata": {
        "id": "bd4bfa9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy.fft\n",
        "import glob as gb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3753bd8f",
      "metadata": {
        "id": "3753bd8f"
      },
      "outputs": [],
      "source": [
        "A=[[0]*5]*5 #upload the dataset here\n",
        "#numeric_cols=[0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f184fed2",
      "metadata": {
        "id": "f184fed2"
      },
      "outputs": [],
      "source": [
        "# importing minmax scaler \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# model creation \n",
        "mm_scaler = MinMaxScaler()\n",
        "# fitting and transforming the model on A(train_inputs)\n",
        "A_mm = pd.DataFrame(mm_scaler.fit_transform(A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d82572",
      "metadata": {
        "id": "b8d82572"
      },
      "outputs": [],
      "source": [
        "# importing Standard Scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "s_scaler = StandardScaler()\n",
        "\n",
        "A_s = pd.DataFrame(s_scaler.fit_transform(A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16fa44e5",
      "metadata": {
        "id": "16fa44e5"
      },
      "outputs": [],
      "source": [
        "# Importing MaxAbs Scaler from sklearn\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "# Modelling the scaler\n",
        "ab_scaler = MaxAbsScaler()\n",
        "# fitting and transforming the model on A(train_inputs)\n",
        "A_ab = pd.DataFrame(ab_scaler.fit_transform(A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "862d7522",
      "metadata": {
        "id": "862d7522"
      },
      "outputs": [],
      "source": [
        "# Importing Robust Scaler from sklearn.preprocessing\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "# Model Creation\n",
        "r_scaler = RobustScaler()\n",
        "# fitting and transforming the model on A(train_inputs)\n",
        "A_r = pd.DataFrame(r_scaler.fit_transform(A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05111a3",
      "metadata": {
        "id": "b05111a3",
        "outputId": "d7560053-54d3-4bea-ec09-74e8adcaacde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\AYUSHMAN\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:2612: UserWarning: n_quantiles (1000) is greater than the total number of samples (5). n_quantiles is set to n_samples.\n",
            "  warnings.warn(\"n_quantiles (%s) is greater than the total number \"\n"
          ]
        }
      ],
      "source": [
        "# Importing Quantile Transformer Scaler from sklearn.preprocessing\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "# Model Creation\n",
        "q_scaler = QuantileTransformer()\n",
        "# fitting and transforming the model on A(train_inputs)\n",
        "A_q = pd.DataFrame(q_scaler.fit_transform(A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d6de86",
      "metadata": {
        "id": "80d6de86",
        "outputId": "769ed143-991c-48dd-d863-1a7f5e606720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\AYUSHMAN\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3237: RuntimeWarning: divide by zero encountered in log\n",
            "  loglike = -n_samples / 2 * np.log(x_trans.var())\n"
          ]
        }
      ],
      "source": [
        "# Importing Power Transformer Scaler from sklearn.preprocessing\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "# Model Creation\n",
        "p_scaler = PowerTransformer(method='yeo-johnson')\n",
        "# fitting and transforming the model on A(train_inputs)\n",
        "A_p = pd.DataFrame(p_scaler.fit_transform(A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7a69c9",
      "metadata": {
        "id": "ef7a69c9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Dataset_scaling.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}